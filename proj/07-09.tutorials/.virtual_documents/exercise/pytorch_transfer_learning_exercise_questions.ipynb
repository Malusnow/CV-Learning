





import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models

print(f"PyTorch Version: {torch.__version__}")
print(f"Torchvision Version: {torchvision.__version__}")





# TODO: Define transformations for the dataset
# The mean and std for ImageNet are [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225]
transform = transforms.Compose([
    # Your code here
    raise NotImplementedError("Define the data transformations."),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Load the datasets
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2) # Using a smaller batch size due to larger image size

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')





# TODO: Load the pretrained VGG16 model
model = None # Your code here
raise NotImplementedError("Load the pretrained VGG16 model.")

# TODO: Freeze the feature extractor layers
# Your code here
raise NotImplementedError("Freeze the feature extractor.")

# TODO: Replace the classifier
# The original VGG16 classifier's last layer is at index 6
num_features = model.classifier[6].in_features
model.classifier[6] = None # Your code here
raise NotImplementedError("Replace the classifier.")

# Move the model to the correct device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)

print(model)
print(f'Model is on device: {next(model.parameters()).device}')





# TODO: Define the loss function and optimizer
criterion = None # Your code here
optimizer = None # Your code here

raise NotImplementedError("Define criterion and optimizer.")





def train(epoch):
    print(f'Epoch: {epoch}')
    model.train()
    train_loss = 0
    correct = 0
    total = 0
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        inputs, targets = inputs.to(device), targets.to(device)
        
        # TODO: Complete the training steps (zero grad, forward, loss, backward, step)
        raise NotImplementedError("Implement the training steps.")

        train_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

        if batch_idx % 100 == 0:
            print(f'Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})')

def test():
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(testloader):
            inputs, targets = inputs.to(device), targets.to(device)
            # TODO: Complete the testing steps (forward pass and loss calculation)
            raise NotImplementedError("Implement the testing steps.")

            test_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

    print(f'Test Loss: {test_loss/len(testloader):.3f} | Test Acc: {100.*correct/total:.3f}%')

for epoch in range(3): # Train for 3 epochs for demonstration
    train(epoch)
    test()



