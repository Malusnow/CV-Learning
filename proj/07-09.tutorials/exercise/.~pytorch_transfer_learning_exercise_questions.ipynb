{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with a Pretrained VGG Network\n",
    "\n",
    "**Objective:** In this exercise, you will learn how to perform transfer learning by using a VGG network pretrained on ImageNet and fine-tuning it for the CIFAR-10 dataset.\n",
    "\n",
    "This notebook will cover:\n",
    "1.  Loading a pretrained model from `torchvision.models`.\n",
    "2.  Adapting the input data to the pretrained model's requirements.\n",
    "3.  Freezing the weights of the convolutional base to leverage learned features.\n",
    "4.  Replacing the model's classifier for a new dataset.\n",
    "5.  Training only the new classifier for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preparing the CIFAR-10 Dataset\n",
    "\n",
    "Pretrained models like VGG were trained on ImageNet and expect input images of a specific size (224x224) and normalization. We must process our CIFAR-10 images (32x32) to match these requirements.\n",
    "\n",
    "**Your Task:** Define the `transform` pipeline. It must:\n",
    "1. Resize the images to 224x224.\n",
    "2. Convert them to PyTorch Tensors.\n",
    "3. Normalize them with the standard ImageNet mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define transformations for the dataset\n",
    "# The mean and std for ImageNet are [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose([\n",
    "    # Your code here\n",
    "    raise NotImplementedError(\"Define the data transformations.\"),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load the datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2) # Using a smaller batch size due to larger image size\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained VGG Model and Adapt for CIFAR-10\n",
    "\n",
    "We will load a VGG16 model pretrained on ImageNet. Then, we will perform two key steps:\n",
    "1.  **Freeze the feature extractor:** The convolutional layers have already learned powerful features. We will freeze them to prevent their weights from being updated during training.\n",
    "2.  **Replace the classifier:** The original classifier was trained for 1000 ImageNet classes. We need to replace it with a new one for our 10 CIFAR-10 classes.\n",
    "\n",
    "**Your Task:** \n",
    "1. Load the pretrained `vgg16` model.\n",
    "2. Freeze the parameters of the `features` part of the model by setting `requires_grad` to `False`.\n",
    "3. Replace the final layer of the `classifier` with a new `nn.Linear` layer suitable for 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the pretrained VGG16 model\n",
    "model = None # Your code here\n",
    "raise NotImplementedError(\"Load the pretrained VGG16 model.\")\n",
    "\n",
    "# TODO: Freeze the feature extractor layers\n",
    "# Your code here\n",
    "raise NotImplementedError(\"Freeze the feature extractor.\")\n",
    "\n",
    "# TODO: Replace the classifier\n",
    "# The original VGG16 classifier's last layer is at index 6\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = None # Your code here\n",
    "raise NotImplementedError(\"Replace the classifier.\")\n",
    "\n",
    "# Move the model to the correct device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(f'Model is on device: {next(model.parameters()).device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Loss Function and Optimizer\n",
    "\n",
    "We will use `CrossEntropyLoss` and the `Adam` optimizer. Importantly, we only want to train the parameters of the new classifier, not the frozen layers.\n",
    "\n",
    "**Your Task:** Instantiate the loss function and the optimizer. Make sure the optimizer is only passed the parameters of the classifier that need to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the loss function and optimizer\n",
    "criterion = None # Your code here\n",
    "optimizer = None # Your code here\n",
    "\n",
    "raise NotImplementedError(\"Define criterion and optimizer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Test the Network\n",
    "\n",
    "Now we will write the training and testing loops. This process should be much faster than training from scratch because we are only updating the weights of the small classifier part of the network.\n",
    "\n",
    "**Your Task:** Complete the training and testing loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # TODO: Complete the training steps (zero grad, forward, loss, backward, step)\n",
    "        raise NotImplementedError(\"Implement the training steps.\")\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            # TODO: Complete the testing steps (forward pass and loss calculation)\n",
    "            raise NotImplementedError(\"Implement the testing steps.\")\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print(f'Test Loss: {test_loss/len(testloader):.3f} | Test Acc: {100.*correct/total:.3f}%')\n",
    "\n",
    "for epoch in range(3): # Train for 3 epochs for demonstration\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bonus Questions\n",
    "\n",
    "1.  What happens if you don't freeze the convolutional layers? How does it affect training time and accuracy? (Hint: You would pass `model.parameters()` to the optimizer).\n",
    "2.  Try fine-tuning more than just the classifier. For example, unfreeze the last convolutional block of `model.features` and add its parameters to the optimizer. You might want to use a smaller learning rate for these layers.\n",
    "3.  Experiment with a different pretrained model, like `resnet18`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
